#!/usr/bin/env python
import datetime
import json
import logging
import os
import re
import subprocess
import time

from glob import glob
from py.path import local as localpath

_DEFAULT_LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
lgr = logging.getLogger(__name__)

# Time to wait before starting to process a directory
PROCESSING_WAIT_TIME = 5 * 3600  # in seconds
MONITOR_SLEEP_TIME = 1 # 0

#def _configure_logging():
lgr.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter(_DEFAULT_LOG_FORMAT)
ch.setFormatter(formatter)
lgr.addHandler(ch)


class TheDB(object):
    """Helper DB to persistently store information about processed directories

    internally it is just a dictionary of "fpath: record" where record is
    a dict containing status, timestamp, output_path, subject, session
    """

    __db_version__ = '0.1'

    def __init__(self, data={}, toppath=None, path=None):
        self._data = {}
        self._toppath = toppath or ""
        self.path = path
        if path and os.path.exists(path):
            self.load(path)
        if data:
            self._data.update(data)

    def keys(self):
        return self._data.keys()

    def __contains__(self, fpath):
        return fpath in self._data

    def get(self, fpath, *args, **kwargs):
        return self._data.get(
            self._get_db_path(fpath), *args, **kwargs)

    def _get_db_path(self, fpath):
        if os.path.isabs(fpath) and self._toppath:
            fpath = os.path.relpath(fpath, self._toppath)
        return fpath

    def __getitem__(self, fpath):
        return self.get(fpath)

    def __setitem__(self, fpath, v):
        self._data[self._get_db_path(fpath)] = v
        # for persistency if path was provided
        # TODO: might want some delay from previous save
        if self.path:
            self.save(self.path)

    def save(self, fpath=None):
        """Save DB as a JSON file
        """
        db = {
            'version': self.__db_version__,
            self._toppath: self._data
        }
        tmp_fpath = fpath + '__'
        if os.path.exists(tmp_fpath):
            lgr.warning("Detected previous incompletely saved file %s, removing",
                        tmp_fpath)
        with open(tmp_fpath, 'w') as f:
            json.dump(db, f, indent=2, sort_keys=True, separators=(',', ': '))
        os.rename(tmp_fpath, fpath)
        return self  # for easier chaining

    def load(self, fpath):
        with open(fpath) as f:
            db = json.load(f)
        if db.get('version') != self.__db_version__:
            raise ValueError("Loaded db from %s is of unsupported version %s. "
                             "Currently supported: %s"
                             % (fpath, db.get('version'), self.__db_version__))
        self._data = db.get(self._toppath, {})
        return self  # for easier chaining


def run_command(cmd):
    info_dict = dict()
    proc = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    return_code = proc.wait()
    if return_code == 0:
        lgr.info("Done running {0}".format(cmd))
        info_dict['status'] = 'completed-ok'
    else:   
        lgr.error("{0} failed".format(cmd))
        info_dict['status'] = 'completed-failed'
    info_dict['timestamp'] = time.time()
    # get info on what we run
    stdout = proc.communicate()[0].decode('utf-8')
    match = re.match("INFO: PROCESSING STARTS: (.*)", stdout)
    info_dict_ = eval(match.group(1) if match else "dict()")
    info_dict.update(info_dict_)
    return stdout, info_dict
    

def process(paths2process, db, wait=PROCESSING_WAIT_TIME, logdir='log'):
    # cmd = 'ls -l {0}'  # TODO: replace with a real command
    cmd = 'heudiconv -o /tmp/out {0}'  # TODO: replace with a real command
    for path in paths2process:
        mod_time = os.path.getmtime(path)
        age = time.time() - mod_time
        if age < wait:
            continue
        record = db.get(path, {})
        if record:
            status = record.get('status')
            if status.startswith('completed'):
                # should not happen really
                lgr.warning(
                    "Path %s was already processed (%s) on %s. "
                    "Skipping re-processing",
                    path, status, time.localtime(record.get('timestamp', 0))
                )
                continue
        cmd_ = cmd.format(path)
        process_record = {
            # TODO? possibly store the relative path?
            'input_path': path,
            'status': 'detected',
            'accession_number': os.path.basename(path)
        }
        lgr.info("Time to process {0} which aged for {1:.2f} seconds".format(path, age))
        output, run_record = run_command(cmd_)
        process_record.update(run_record)
        db[path] = process_record

        # save log
        logdir = localpath(logdir)
        logfile = process_record['accession_number'] + '.log'
        if os.path.exists(logfile):
            lgr.warning("Log file for accession already exists: %s", logfile)
        log = logdir.join(logfile)
        log.write(output, 'a')


def get_directories_from_start_date(topdir, db, startdate):
    """Given toppath, would return directories assuming YEAR/MONTH/DATE/SMTH
    layout from the startdate.  It must process the db and status in the records
    and NOT output any directory which already has a "completed-*" or "skip"
    record"""
    now = datetime.datetime.now()
    if not startdate:
        startdate = datetime.datetime(2017, 3, 7)
    elif isinstance(startdate, (float, int)):
        startdate = datetime.datetime.fromtimestamp(startdate)
    lgr.debug("Getting directories from date %s till %s", startdate, now)
    # Go through all possible date'd directories and if exist collect their sub
    # directories
    # quantize to day without hour:min
    day = datetime.datetime(startdate.year, startdate.month, startdate.day)
    paths = []
    while day <= now:
        daypath = os.path.join("{0}/{1.year}/{1.month:02d}/{1.day:02d}".format(topdir, day))
        lgr.debug("Considering date %s, path: %s", day, daypath)
        daypaths = glob(os.path.join(daypath, '*'))
        if daypaths:
            lgr.debug("Got %d paths for the date %s", len(daypaths), day)
        paths.extend(daypaths)
        day += datetime.timedelta(days=1)
    # paths = [os.path.join(topdir, '2017/03/07/A000TEMP')]
    paths_out = [
        path for path in paths
        if db.get(path, {}).get('status', '') not in {'completed-ok', 'completed-failed', 'skip'}
    ]
    lgr.debug("Filtered the list of %d paths based on status to have %d entries", len(paths), len(paths_out))
    return paths_out


def get_latest_date_from_db(db):
    # go through all the records, parse their paths
    # topdir is needed in case if we store full paths
    date = 0
    for p, r in (db._data or {}).items():
        date = max(0, r.get('timestamp'))
    return date


def yield_relevant_directories(topdir, db, start_date):
    """

    Parameters
    ----------
    topdir
    db
    start_date: datetime

    Returns
    -------

    """
    # figure out the days to monitor and create a list of those
    # mtimes = {}  # dir: (mtime)
    while True:
        dirs = get_directories_from_start_date(topdir, db, start_date)
        if not dirs:
            # nothing left to process given current state of db
            # so we exit the generator so outside loop could "refresh" its date
            # etc
            # Give it some rest since there were nothing to process really
            # and theoretically we could sleep for PROCESSING_WAIT_TIME
            lgr.debug("Sleeping tight before breaking")
            time.sleep(MONITOR_SLEEP_TIME)
            break
        yield dirs
        lgr.debug("Sleeping tight just for the sake of it")
        time.sleep(MONITOR_SLEEP_TIME)


def monitor(topdir='/tmp/new_dir', db=None, wait=PROCESSING_WAIT_TIME, logdir='log'):
    # make logdir if not existant
    try:
        os.makedirs(logdir)
    except OSError:
        pass

    while True:
        # given our records, from what date should we monitor directories
        start_date = get_latest_date_from_db(db)

        for dirs in yield_relevant_directories(topdir, db, start_date):
            process(dirs, db,  wait=wait, logdir=logdir)


def parse_args():
    import argparse
    parser = argparse.ArgumentParser(prog='monitor.py', description='Small monitoring script to detect new directories and process them', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('path', help='Which directory to monitor')
    parser.add_argument('--database', '-d', help='database location', default='database.json')
    parser.add_argument('--wait_time', '-w', help='After how long should we start processing datasets? (in seconds)', default=86400, type=float)
    parser.add_argument('--logdir', '-l', help='Where should we save the logs?', default='log')


    return parser.parse_args()


if __name__ == '__main__':
    parsed = parse_args()
    # open database
    db = TheDB(toppath=parsed.path, path=parsed.database) # , default_table='heudiconv')
    monitor(parsed.path, db, wait=parsed.wait_time, logdir=parsed.logdir)
